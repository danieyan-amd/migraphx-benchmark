name: ONNX Model Zoo Tests (Docker-light)

on:
  workflow_call:
    inputs:
      rocm_release:        { type: string, required: true }
      onnx_reports_repo:   { type: string, required: true }
      benchmark_utils_repo:{ type: string, required: true }
      organization:        { type: string, required: true }
      result_number:       { type: string, required: true }
      model_timeout:       { type: string, required: true }
    secrets:
      gh_token: { required: true }

permissions:
  contents: read

env:
  DATASETS_DIR: /usr/share/migraphx/migraph_datasets
  LOG_ROOT: logs
  DOCKER_IMAGE: rocm/pytorch:latest
  PYTHONPATH: /opt/rocm/lib
  LD_LIBRARY_PATH: /opt/rocm/lib

jobs:
  model-zoo-tests:
    runs-on: [self-hosted, Linux, X64]
    timeout-minutes: 90

    steps:
      - name: Checkout sources
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.gh_token }}

      - name: Init timestamp
        id: ts
        shell: bash
        run: echo "stamp=$(date -u +%Y%m%dT%H%M%SZ)" >> "$GITHUB_OUTPUT"

      # (Host) Sanity: show whatâ€™s under the usual dataset roots
      - name: List datasets on host (debug)
        shell: bash
        run: |
          set -e
          for d in \
            /usr/share/migraphx/migraph_datasets \
            /usr/share/migraphx/saved-models \
            /usr/share/migraphx/new-saved-models
          do
            echo "==> Host path: $d"
            if [ -d "$d" ]; then
              echo "    exists"
              # Count a few candidate files
              cnt=$(find "$d" -type f \( -name '*.onnx' -o -name '*.pb' \) | head -100 | wc -l)
              echo "    sample model files found: $cnt"
            else
              echo "    (missing)"
            fi
          done

      - name: Run ONNX Model Zoo in container
        shell: bash
        env:
          RUN_TS: ${{ steps.ts.outputs.stamp }}
        run: |
          set -euo pipefail

          docker run --rm \
            --name model-zoo-${GITHUB_RUN_ID} \
            --device=/dev/kfd --device=/dev/dri --group-add=video \
            --network=host \
            -e TARGET=gpu \
            -e ATOL=0.001 -e RTOL=0.001 \
            -e RUN_TS="${RUN_TS}" \
            -e PYTHONPATH=${PYTHONPATH} \
            -e LD_LIBRARY_PATH=${LD_LIBRARY_PATH} \
            -v /opt/rocm:/opt/rocm:ro \
            -v "/usr/share/migraphx/migraph_datasets":/datasets:ro \
            -v /usr/share/migraphx/saved-models:/saved-models:ro \
            -v /usr/share/migraphx/new-saved-models:/new-saved-models:ro \
            -v "$GITHUB_WORKSPACE":/workspace \
            -w /workspace \
            "${{ env.DOCKER_IMAGE }}" \
            bash -lc '
              set -euo pipefail

              echo "Checking dataset candidates inside container..."
              for d in /datasets /saved-models /new-saved-models; do
                if [ -d "$d" ]; then
                  c=$(find "$d" -type f \( -name "*.onnx" -o -name "*.pb" \) | head -100 | wc -l)
                  printf "  %s: %s (sample files: %s)\n" "$d" "present" "$c"
                else
                  printf "  %s: missing\n" "$d"
                fi
              done

              # Pick a dataset root that actually has model files
              DATASET_ROOT=""
              for cand in /datasets /saved-models /new-saved-models; do
                if [ -d "$cand" ] && find "$cand" -type f \( -name "*.onnx" -o -name "*.pb" \) | head -1 | grep -q .; then
                  DATASET_ROOT="$cand"
                  break
                fi
              done

              if [ -z "${DATASET_ROOT:-}" ]; then
                echo "ERROR: No model files found under /datasets, /saved-models, or /new-saved-models."
                exit 1
              fi
              echo "Using DATASET_ROOT=$DATASET_ROOT"

              # Install Node.js 20 for the JS generator (if missing)
              if ! command -v node >/dev/null 2>&1; then
                apt-get update -y
                apt-get install -y --no-install-recommends ca-certificates curl gnupg
                install -d /etc/apt/keyrings
                curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg
                echo "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_20.x nodistro main" > /etc/apt/sources.list.d/nodesource.list
                apt-get update -y && apt-get install -y --no-install-recommends nodejs
              fi
              node --version

              # Install test-only deps (skip torch/onnxruntime to avoid clashes with base)
              if [ -f tools/model_zoo/test_generator/requirements.txt ]; then
                grep -viE "^(torch|torchvision|torchaudio|onnxruntime(-gpu)?)\\b" tools/model_zoo/test_generator/requirements.txt > .req_notorch.txt || true
                pip3 install -r .req_notorch.txt
              fi

              chmod +x tools/model_zoo/test_generator/test_models.sh || true
              chmod +x ./run-model-zoo-tests.js || true

              ROOT="logs/${RUN_TS}"
              mkdir -p "${ROOT}/fp16" "${ROOT}/fp32"

              echo "Generating cases from datasets: $DATASET_ROOT"
              node --trace-uncaught --unhandled-rejections=strict \
                ./run-model-zoo-tests.js \
                --datasets "$DATASET_ROOT" \
                --out tools/model_zoo/onnx_zoo

              echo "Running model-zoo tests..."
              bash tools/model_zoo/test_generator/test_models.sh tools/model_zoo/onnx_zoo 2>&1 | tee "${ROOT}/raw.log"

              shopt -s nullglob
              for p in fp16 fp32; do
                src="tools/model_zoo/test_generator/logs/$p"
                [ -d "$src" ] && cp -f "$src"/*.log "${ROOT}/$p/" || true
              done

              # ---- Write JS to a temp file and run it (avoids nested-quote errors) ----
              cat >/tmp/onnx_summarize.js <<'"'"'JS'"'"'
              const fs = require("fs");
              const path = require("path");

              const runTs = process.env.RUN_TS;
              const root = path.join("logs", runTs);
              const precs = ["fp32", "fp16"];
              const summary = { totals: { pass: 0, fail: 0 }, regressions: {} };

              function looksFailed(text) {
                return /(Traceback \(most recent call last\)|\bERROR\b|AssertionError|Segmentation fault|^error:)/im.test(text);
              }
              function failureMessage(text) {
                const tb = [...text.matchAll(/Traceback \(most recent call last\):([\s\S]*?)(?:\n\s*\n|\Z)/g)];
                if (tb.length) {
                  const block = tb[tb.length - 1][1].trim().split(/\r?\n/).reverse();
                  for (const line of block) if (line.trim()) return line.trim();
                }
                const lines = text.split(/\r?\n/).map(l => l.trim()).filter(Boolean).reverse();
                for (const l of lines) if (/(error|exception|failed|segmentation fault|assert)/i.test(l)) return l;
                return "failed (see log)";
              }

              for (const prec of precs) {
                const d = path.join(root, prec);
                const reg = { passed: [], failed: [] };
                if (fs.existsSync(d) && fs.statSync(d).isDirectory()) {
                  const files = fs.readdirSync(d).filter(f => f.endsWith(".log")).sort();
                  for (const fn of files) {
                    const model = fn.slice(0, -4);
                    const txt = fs.readFileSync(path.join(d, fn), "utf8");
                    if (looksFailed(txt)) {
                      reg.failed.push({ model, message: failureMessage(txt) });
                      summary.totals.fail += 1;
                    } else {
                      reg.passed.push(model);
                      summary.totals.pass += 1;
                    }
                  }
                }
                summary.regressions[prec] = reg;
              }

              fs.writeFileSync(path.join(root, "summary.json"), JSON.stringify(summary, null, 2));

              const lines = [];
              lines.push("## Totals");
              lines.push(`- PASS: ${summary.totals.pass}`);
              lines.push(`- FAIL: ${summary.totals.fail}`);
              lines.push("");
              for (const prec of precs) {
                const reg = summary.regressions[prec];
                lines.push(`## ${prec.toUpperCase()}`);
                lines.push(`**Passed (${reg.passed.length})**`);
                if (reg.passed.length) reg.passed.forEach(m => lines.push(`- ${m}`)); else lines.push("- none");
                lines.push("");
                lines.push(`**Failed (${reg.failed.length})**`);
                if (reg.failed.length) reg.failed.forEach(it => lines.push(`- ${it.model}: \`${it.message}\``)); else lines.push("- none");
                lines.push("");
              }
              fs.writeFileSync(path.join(root, "summary.md"), lines.join("\n"));
              JS

              node /tmp/onnx_summarize.js
            '

      - name: Upload logs and summary (timestamped)
        uses: actions/upload-artifact@v4
        with:
          name: model-zoo-logs-${{ steps.ts.outputs.stamp }}
          path: logs/${{ steps.ts.outputs.stamp }}
          if-no-files-found: error
