name: ONNX Model Zoo Tests

on:
  workflow_call:
    inputs:
      rocm_release:
        description: ROCm Version
        required: true
        type: string
      onnx_reports_repo:
        description: Repository where ONNX reports are stored
        required: true
        type: string
      benchmark_utils_repo:
        description: Repository where benchmark utils are stored
        required: true
        type: string
      organization:
        description: Organization name
        required: true
        type: string
      result_number:
        description: Last N results to compare
        required: true
        type: string
      model_timeout:
        description: Skip models exceeding this runtime (e.g. "30m")
        required: true
        type: string
    secrets:
      gh_token:
        description: Token with repo permissions
        required: true
      mail_user:
        required: true
      mail_pass:
        required: true

jobs:
  model-zoo-tests:
    runs-on: [self-hosted, Linux, X64]
    timeout-minutes: 60

    env:
      DATASETS_DIR: /usr/share/migraphx/migraph_datasets
      # make migraphx Python binding visible
      PYTHONPATH: /opt/rocm/lib:${PYTHONPATH}
      LD_LIBRARY_PATH: /opt/rocm/lib:${LD_LIBRARY_PATH}

    steps:
      - uses: actions/checkout@v3
        with:
          token: ${{ secrets.gh_token }}

      - uses: actions/setup-node@v3
        with:
          node-version: 20

      - uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install test dependencies
        run: pip install -r tools/model_zoo/test_generator/requirements.txt

      - name: Init timestamp
        id: ts
        run: echo "stamp=$(date -u +%Y%m%dT%H%M%SZ)" >> "$GITHUB_OUTPUT"

      - name: Ensure scripts are executable
        run: |
          chmod +x tools/model_zoo/test_generator/test_models.sh
          chmod +x ./run-model-zoo-tests.js

      - name: Generate cases from datasets (repo-root JS)
        run: |
          set -e
          echo "runner.name=${{ runner.name }}"
          hostname
          ls -ld "${DATASETS_DIR}"
          node ./run-model-zoo-tests.js \
            --datasets "${DATASETS_DIR}" \
            --out tools/model_zoo/onnx_zoo

      - name: Run model-zoo tests and capture logs
        env:
          TARGET: gpu
          ATOL: 0.001
          RTOL: 0.001
          RUN_TS: ${{ steps.ts.outputs.stamp }}
          PYTHONPATH: /opt/rocm/lib:${PYTHONPATH}
          LD_LIBRARY_PATH: /opt/rocm/lib:${LD_LIBRARY_PATH}
        run: |
          set -e

          ROOT="logs/${RUN_TS}"
          mkdir -p "${ROOT}/fp16" "${ROOT}/fp32"
          echo "${RUN_TS}" > "${ROOT}/run.timestamp"

          echo "Testing models in tools/model_zoo/onnx_zoo"
          bash tools/model_zoo/test_generator/test_models.sh tools/model_zoo/onnx_zoo 2>&1 | tee "${ROOT}/raw.log"

          # Copy per-model logs into our timestamped folder
          shopt -s nullglob
          for p in fp16 fp32; do
            src="tools/model_zoo/test_generator/logs/$p"
            [ -d "$src" ] && cp -f "$src"/*.log "${ROOT}/$p/" || true
          done

          rm -f logs/latest && ln -s "${RUN_TS}" logs/latest

      - name: Summarize results (totals, per-precision pass list, fail list with message)
        env:
          RUN_TS: ${{ steps.ts.outputs.stamp }}
        run: |
          set -e
          ROOT="logs/${RUN_TS}"
          OUT_MD="${ROOT}/summary.md"
          OUT_JSON="${ROOT}/summary.json"

          node - <<'JS'
          const fs = require('fs');
          const path = require('path');

          const runTs = process.env.RUN_TS;
          const root = path.join('logs', runTs);
          const precs = ['fp32','fp16'];
          const summary = { totals: { pass: 0, fail: 0 }, regressions: {} };

          function looksFailed(text) {
            return /(Traceback \(most recent call last\)|\bERROR\b|AssertionError|Segmentation fault|^error:)/im.test(text);
          }
          function failureMessage(text) {
            const tb = [...text.matchAll(/Traceback \(most recent call last\):([\s\S]*?)(?:\n\s*\n|\Z)/g)];
            if (tb.length) {
              const block = tb[tb.length-1][1].trim().split(/\r?\n/).reverse();
              for (const line of block) if (line.trim()) return line.trim();
            }
            const lines = text.split(/\r?\n/).map(l => l.trim()).filter(Boolean).reverse();
            for (const l of lines) if (/(error|exception|failed|segmentation fault|assert)/i.test(l)) return l;
            return 'failed (see log)';
          }

          for (const prec of precs) {
            const d = path.join(root, prec);
            const reg = { passed: [], failed: [] };
            if (fs.existsSync(d) && fs.statSync(d).isDirectory()) {
              const files = fs.readdirSync(d).filter(f => f.endsWith('.log')).sort();
              for (const fn of files) {
                const model = fn.slice(0, -4);
                const txt = fs.readFileSync(path.join(d, fn), 'utf8');
                if (looksFailed(txt)) {
                  reg.failed.push({ model, message: failureMessage(txt) });
                  summary.totals.fail += 1;
                } else {
                  reg.passed.push(model);
                  summary.totals.pass += 1;
                }
              }
            }
            summary.regressions[prec] = reg;
          }

          fs.writeFileSync(path.join(root, 'summary.json'), JSON.stringify(summary, null, 2));

          const lines = [];
          lines.push('## Totals');
          lines.push(`- PASS: ${summary.totals.pass}`);
          lines.push(`- FAIL: ${summary.totals.fail}`);
          lines.push('');
          for (const prec of precs) {
            const reg = summary.regressions[prec];
            lines.push(`## ${prec.toUpperCase()}`);
            lines.push(`**Passed (${reg.passed.length})**`);
            if (reg.passed.length) reg.passed.forEach(m => lines.push(`- ${m}`)); else lines.push('- none');
            lines.push('');
            lines.push(`**Failed (${reg.failed.length})**`);
            if (reg.failed.length) reg.failed.forEach(it => lines.push(`- ${it.model}: \`${it.message}\``)); else lines.push('- none');
            lines.push('');
          }
          const md = lines.join('\n');
          fs.writeFileSync(path.join(root, 'summary.md'), md);
          if (process.env.GITHUB_STEP_SUMMARY) {
            fs.appendFileSync(process.env.GITHUB_STEP_SUMMARY, md);
          }
          JS

          echo "Summary written to ${OUT_MD} and ${OUT_JSON}"

      - name: Upload logs and summary (timestamped)
        uses: actions/upload-artifact@v4
        with:
          name: model-zoo-logs-${{ steps.ts.outputs.stamp }}
          path: logs/${{ steps.ts.outputs.stamp }}
          if-no-files-found: error
