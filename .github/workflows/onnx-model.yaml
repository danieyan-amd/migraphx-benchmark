name: ONNX Model Zoo Tests (Docker)

on:
  workflow_call:
    inputs:
      rocm_release:
        type: string
        required: true
      onnx_reports_repo:
        type: string
        required: true
      benchmark_utils_repo:
        type: string
        required: true
      organization:
        type: string
        required: true
      result_number:
        type: string
        required: true
      model_timeout:
        type: string
        required: true
    secrets:
      gh_token:
        required: true
      mail_user:
        required: true
      mail_pass:
        required: true

permissions:
  contents: read

env:
  DATASETS_DIR: /usr/share/migraphx/migraph_datasets
  DOCKER_IMAGE: onnx-model:${{ github.sha }}

jobs:
  model-zoo-tests:
    runs-on: [self-hosted, Linux, X64, gpu942f]
    timeout-minutes: 60

    steps:
      - name: Checkout sources (caller repo)
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.gh_token }}
          fetch-depth: 1

      # Adjust -f if your Dockerfile isnâ€™t at repo root
      - name: Build MIGraphX image
        shell: bash
        run: |
          set -euo pipefail
          docker build -t "$DOCKER_IMAGE" -f Dockerfile .

      - name: Init timestamp
        id: ts
        shell: bash
        run: |
          set -euo pipefail
          echo "stamp=$(date -u +%Y%m%dT%H%M%SZ)" >> "$GITHUB_OUTPUT"

      - name: Run ONNX model tests inside container
        shell: bash
        env:
          RUN_TS: ${{ steps.ts.outputs.stamp }}
        run: |
          set -euo pipefail
          mkdir -p "logs/${RUN_TS}"

          # Define these at runtime to avoid ${...} in YAML scalars
          export PYTHONPATH="/opt/rocm/lib:${PYTHONPATH:-}"
          export LD_LIBRARY_PATH="/opt/rocm/lib:${LD_LIBRARY_PATH:-}"

          docker run --rm \
            --name "onnx-model-${GITHUB_RUN_ID}" \
            --device=/dev/dri --device=/dev/kfd \
            --network=host --group-add=video \
            -e TARGET=gpu \
            -e ATOL=0.001 \
            -e RTOL=0.001 \
            -e RUN_TS="${RUN_TS}" \
            -e PYTHONPATH="/opt/rocm/lib:${PYTHONPATH:-}" \
            -e LD_LIBRARY_PATH="/opt/rocm/lib:${LD_LIBRARY_PATH:-}" \
            -v "${DATASETS_DIR}:/datasets:ro" \
            -v "$GITHUB_WORKSPACE:/workspace" \
            -w /workspace \
            "$DOCKER_IMAGE" \
            bash -lc "
              set -euo pipefail

              # Install repo-pinned Python deps (inside container)
              if [ -f tools/model_zoo/test_generator/requirements.txt ]; then
                pip3 install -r tools/model_zoo/test_generator/requirements.txt
              fi

              # Ensure scripts & logs dirs exist
              chmod +x tools/model_zoo/test_generator/test_models.sh || true
              chmod +x ./run-model-zoo-tests.js || true
              mkdir -p 'logs/${RUN_TS}/fp16' 'logs/${RUN_TS}/fp32'

              echo 'Datasets at /datasets'
              node ./run-model-zoo-tests.js --datasets /datasets --out tools/model_zoo/onnx_zoo

              echo 'Running model zoo tests...'
              bash tools/model_zoo/test_generator/test_models.sh tools/model_zoo/onnx_zoo 2>&1 | tee 'logs/${RUN_TS}/raw.log'

              # Copy per-precision logs
              shopt -s nullglob
              for p in fp16 fp32; do
                src=\"tools/model_zoo/test_generator/logs/\$p\"
                [ -d \"\$src\" ] && cp -f \"\$src\"/*.log 'logs/${RUN_TS}/'\"\$p\"'/' || true
              done

              # Write Node summary to a file (avoids here-doc quoting in YAML)
              cat > .summary.js <<'EOS'
              const fs = require('fs');
              const path = require('path');
              const runTs = process.env.RUN_TS;
              const root = path.join('logs', runTs);
              const precs = ['fp32','fp16'];
              const summary = { totals:{pass:0,fail:0}, regressions:{} };

              function looksFailed(text){
                return /(Traceback \(most recent call last\)|\bERROR\b|AssertionError|Segmentation fault|^error:)/im.test(text);
              }
              function failureMessage(text){
                const tb=[...text.matchAll(/Traceback \(most recent call last\):([\s\S]*?)(?:\n\s*\n|\Z)/g)];
                if(tb.length){
                  const block=tb[tb.length-1][1].trim().split(/\r?\n/).reverse();
                  for(const line of block) if(line.trim()) return line.trim();
                }
                const lines=text.split(/\r?\n/).map(l=>l.trim()).filter(Boolean).reverse();
                for(const l of lines) if (/(error|exception|failed|segmentation fault|assert)/i.test(l)) return l;
                return 'failed (see log)';
              }

              for (const prec of precs){
                const d = path.join(root, prec);
                const reg = { passed: [], failed: [] };
                if (fs.existsSync(d) && fs.statSync(d).isDirectory()){
                  const files = fs.readdirSync(d).filter(f => f.endsWith('.log')).sort();
                  for (const fn of files){
                    const model = fn.slice(0, -4);
                    const txt = fs.readFileSync(path.join(d, fn), 'utf8');
                    if (looksFailed(txt)){ reg.failed.push({ model, message: failureMessage(txt) }); summary.totals.fail++; }
                    else { reg.passed.push(model); summary.totals.pass++; }
                  }
                }
                summary.regressions[prec] = reg;
              }

              fs.writeFileSync(path.join(root, 'summary.json'), JSON.stringify(summary, null, 2));

              const lines = [];
              lines.push('## Totals', `- PASS: ${summary.totals.pass}`, `- FAIL: ${summary.totals.fail}`, '');
              for (const prec of precs){
                const reg = summary.regressions[prec];
                lines.push(`## ${prec.toUpperCase()}`);
                lines.push(`**Passed (${reg.passed.length})**`);
                if (reg.passed.length) reg.passed.forEach(m => lines.push(`- ${m}`)); else lines.push('- none');
                lines.push('');
                lines.push(`**Failed (${reg.failed.length})**`);
                if (reg.failed.length) reg.failed.forEach(it => lines.push(`- ${it.model}: \`${it.message}\``)); else lines.push('- none');
                lines.push('');
              }
              fs.writeFileSync(path.join(root, 'summary.md'), lines.join('\n'));
              EOS

              node .summary.js
            "

      - name: Upload logs and summary
        uses: actions/upload-artifact@v4
        with:
          name: model-zoo-logs-${{ steps.ts.outputs.stamp }}
          path: logs/${{ steps.ts.outputs.stamp }}
          if-no-files-found: error
