name: ONNX Model Zoo Tests (Docker)

on:
  workflow_call:
    inputs:
      rocm_release:
        required: true
        type: string
      onnx_reports_repo:
        required: true
        type: string
      benchmark_utils_repo:
        required: true
        type: string
      organization:
        required: true
        type: string
      result_number:
        required: true
        type: string
      model_timeout:
        required: true
        type: string
    secrets:
      gh_token:
        required: true
      mail_user:
        required: true
      mail_pass:
        required: true

permissions:
  contents: read

env:
  DATASETS_DIR: /usr/share/migraphx/migraph_datasets
  PYTHONPATH: /opt/rocm/lib:${PYTHONPATH}
  LD_LIBRARY_PATH: /opt/rocm/lib:${LD_LIBRARY_PATH}
  DOCKER_IMAGE: onnx-model:${{ github.sha }}

jobs:
  model-zoo-tests:
    runs-on: [self-hosted, Linux, X64, gpu942f]
    timeout-minutes: 60

    steps:
      - name: Checkout sources (caller repo: AMDMIGraphX)
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.gh_token }}
          fetch-depth: 1

      # If your Dockerfile is NOT at repo root, change "-f" to the correct path.
      - name: Build MIGraphX image
        run: |
          docker build -t "$DOCKER_IMAGE" -f Dockerfile .

      - name: Init timestamp
        id: ts
        run: echo "stamp=$(date -u +%Y%m%dT%H%M%SZ)" >> "$GITHUB_OUTPUT"

      - name: Run ONNX model tests inside container
        env:
          RUN_TS: ${{ steps.ts.outputs.stamp }}
        run: |
          # ensure host-side folder exists to collect logs
          mkdir -p "logs/${RUN_TS}"

          docker run --rm \
            --name "onnx-model-${GITHUB_RUN_ID}" \
            --device=/dev/dri --device=/dev/kfd \
            --network=host --group-add=video \
            -e TARGET=gpu \
            -e ATOL=0.001 -e RTOL=0.001 \
            -e RUN_TS="${RUN_TS}" \
            -e PYTHONPATH="${PYTHONPATH}" \
            -e LD_LIBRARY_PATH="${LD_LIBRARY_PATH}" \
            -v "${DATASETS_DIR}:/datasets:ro" \
            -v "$GITHUB_WORKSPACE:/workspace" \
            -w /workspace \
            "$DOCKER_IMAGE" \
            bash -lc '
              set -euo pipefail

              # Make sure Node and Python deps exist inside the container image.
              # If your image already includes Node, this is a no-op; otherwise install it in the image.
              # Ensure Python deps come from your repo:
              if [ -f tools/model_zoo/test_generator/requirements.txt ]; then
                pip3 install -r tools/model_zoo/test_generator/requirements.txt
              fi

              # Ensure scripts are executable and logs dirs exist
              chmod +x tools/model_zoo/test_generator/test_models.sh || true
              chmod +x ./run-model-zoo-tests.js || true
              mkdir -p "logs/${RUN_TS}/fp16" "logs/${RUN_TS}/fp32"

              echo "Datasets at /datasets"
              node ./run-model-zoo-tests.js --datasets /datasets --out tools/model_zoo/onnx_zoo

              echo "Running model zoo tests..."
              bash tools/model_zoo/test_generator/test_models.sh tools/model_zoo/onnx_zoo 2>&1 | tee "logs/${RUN_TS}/raw.log"

              # Copy per-precision logs into the timestamped folder
              shopt -s nullglob
              for p in fp16 fp32; do
                src="tools/model_zoo/test_generator/logs/$p"
                [ -d "$src" ] && cp -f "$src"/*.log "logs/${RUN_TS}/$p/" || true
              done

              # Summarize results
              node - <<'"'"'JS'"'"'
              const fs = require("fs");
              const path = require("path");
              const runTs = process.env.RUN_TS;
              const root = path.join("logs", runTs);
              const precs = ["fp32","fp16"];
              const summary = { totals:{pass:0,fail:0}, regressions:{} };

              function looksFailed(text){
                return /(Traceback \(most recent call last\)|\bERROR\b|AssertionError|Segmentation fault|^error:)/im.test(text);
              }
              function failureMessage(text){
                const tb=[...text.matchAll(/Traceback \(most recent call last\):([\s\S]*?)(?:\n\s*\n|\Z)/g)];
                if(tb.length){
                  const block=tb[tb.length-1][1].trim().split(/\r?\n/).reverse();
                  for(const line of block) if(line.trim()) return line.trim();
                }
                const lines=text.split(/\r?\n/).map(l=>l.trim()).filter(Boolean).reverse();
                for(const l of lines) if (/(error|exception|failed|segmentation fault|assert)/i.test(l)) return l;
                return "failed (see log)";
              }

              for(const prec of precs){
                const d=path.join(root,prec);
                const reg={passed:[],failed:[]};
                if(fs.existsSync(d)&&fs.statSync(d).isDirectory()){
                  const files=fs.readdirSync(d).filter(f=>f.endsWith(".log")).sort();
                  for(const fn of files){
                    const model=fn.slice(0,-4);
                    const txt=fs.readFileSync(path.join(d,fn),"utf8");
                    if(looksFailed(txt)){
                      reg.failed.push({model, message: failureMessage(txt)});
                      summary.totals.fail++;
                    }else{
                      reg.passed.push(model);
                      summary.totals.pass++;
                    }
                  }
                }
                summary.regressions[prec]=reg;
              }

              fs.writeFileSync(path.join(root,"summary.json"), JSON.stringify(summary,null,2));

              const lines=[];
              lines.push("## Totals", `- PASS: ${summary.totals.pass}`, `- FAIL: ${summary.totals.fail}`, "");
              for(const prec of precs){
                const reg=summary.regressions[prec];
                lines.push(`## ${prec.toUpperCase()}`);
                lines.push(`**Passed (${reg.passed.length})**`);
                if(reg.passed.length) reg.passed.forEach(m=>lines.push(`- ${m}`)); else lines.push("- none");
                lines.push("");
                lines.push(`**Failed (${reg.failed.length})**`);
                if(reg.failed.length) reg.failed.forEach(it=>lines.push(`- ${it.model}: \`${it.message}\``)); else lines.push("- none");
                lines.push("");
              }
              fs.writeFileSync(path.join(root,"summary.md"), lines.join("\n"));
              JS
            '

      - name: Upload logs and summary
        uses: actions/upload-artifact@v4
        with:
          name: model-zoo-logs-${{ steps.ts.outputs.stamp }}
          path: logs/${{ steps.ts.outputs.stamp }}
          if-no-files-found: error
