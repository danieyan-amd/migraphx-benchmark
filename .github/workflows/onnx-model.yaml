name: ONNX Model Zoo Tests (ROCm dev base + MIGraphX)

on:
  workflow_call:
    inputs:
      rocm_release:
        required: true
        type: string          # e.g. "6.4.2"
      base_image:
        required: false
        type: string          # override if you want; default below is robust
        default: "rocm/dev-ubuntu-22.04"
      benchmark_utils_repo:
        required: true
        type: string
      onnx_reports_repo:
        required: true
        type: string
      organization:
        required: true
        type: string
      result_number:
        required: true
        type: string
      model_timeout:
        required: true
        type: string
    secrets:
      gh_token:
        required: true
      mail_user:
        required: false
      mail_pass:
        required: false

permissions:
  contents: read

env:
  UTILS_DIR: benchmark-utils
  REPORTS_DIR: migraphx-reports
  DATASETS_DIR: /usr/share/migraphx/saved-models
  TEST_RESULTS_PATH: /usr/share/migraphx/${{ inputs.organization }}/test-results
  REPORTS_PATH: /usr/share/migraphx/${{ inputs.organization }}/reports

jobs:
  onnx_zoo:
    runs-on: [self-hosted, Linux, X64]
    timeout-minutes: 120

    steps:
      - name: Map runner to GPU/CPU
        shell: bash
        run: |
          case "${RUNNER_NAME:-unknown}" in
            GPU942F_1|gpu942f) GPU=0; CPU="0-63" ;;
            GPU942F_2)         GPU=4; CPU="64-127" ;;
            *) echo "Unknown runner ${RUNNER_NAME:-?}, default GPU=0, all CPUs"; GPU=0; CPU="0-$(( $(nproc)-1 ))" ;;
          esac
          echo "GPU=$GPU" >> $GITHUB_ENV
          echo "CPU=$CPU" >> $GITHUB_ENV

      - name: Checkout AMDMIGraphX
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.gh_token }}

      - name: Checkout benchmark-utils
        uses: actions/checkout@v4
        with:
          repository: ${{ inputs.benchmark_utils_repo }}
          path: ${{ env.UTILS_DIR }}
          token: ${{ secrets.gh_token }}

      - name: Write Dockerfile (ROCm dev base)
        run: |
          cat > Dockerfile.onnx << 'DOCKER'
          ARG BASE_IMAGE
          ARG ROCM_REL

          FROM ${BASE_IMAGE}:${ROCM_REL}

          # Tools + Node.js for the JS case generator
          RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
                ca-certificates curl gnupg build-essential git cmake pkg-config python3 python3-pip python3-dev \
                jq bash coreutils findutils sed gawk \
             && install -d /etc/apt/keyrings \
             && curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg \
             && echo 'deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_20.x nodistro main' > /etc/apt/sources.list.d/nodesource.list \
             && apt-get update && apt-get install -y --no-install-recommends nodejs \
             && rm -rf /var/lib/apt/lists/*

          WORKDIR /workspace
          DOCKER

      - name: Build ONNX runner image
        run: |
          docker build \
            --build-arg BASE_IMAGE=${{ inputs.base_image }} \
            --build-arg ROCM_REL=${{ inputs.rocm_release }} \
            -t onnx-rocm-migraphx:${{ github.sha }} \
            -f Dockerfile.onnx .

      - name: Init timestamp
        id: ts
        run: echo "stamp=$(date -u +%Y%m%dT%H%M%SZ)" >> "$GITHUB_OUTPUT"

      - name: Run ONNX tests in container
        env:
          RUN_TS: ${{ steps.ts.outputs.stamp }}
        shell: bash
        run: |
          set -euo pipefail
          ROOT="${{ env.TEST_RESULTS_PATH }}/onnx-${RUN_TS}"
          mkdir -p "$ROOT/fp16" "$ROOT/fp32"

          docker run --rm \
            --name onnx-model-${GITHUB_RUN_ID} \
            --cpuset-cpus=${{ env.CPU }} \
            -e ROCR_VISIBLE_DEVICES=${{ env.GPU }} \
            --device=/dev/dri --device=/dev/kfd \
            --group-add=video --network=host \
            -e RUN_TS=${RUN_TS} \
            -v "$PWD:/workspace:rw" \
            -v "${{ env.DATASETS_DIR }}:/datasets:ro" \
            -v "${{ env.TEST_RESULTS_PATH }}:${{ env.TEST_RESULTS_PATH }}" \
            -v "$PWD/${{ env.UTILS_DIR }}/scripts:/migraphx/sh:ro" \
            onnx-rocm-migraphx:${{ github.sha }} \
            bash -lc '
              set -euo pipefail
              cd /workspace

              # 1) Install Python deps (skip torch/onnxruntime-gpu if present)
              if [ -f tools/model_zoo/test_generator/requirements.txt ]; then
                grep -viE "^(torch|torchvision|torchaudio|onnxruntime-gpu)\b" tools/model_zoo/test_generator/requirements.txt > .req_notorch.txt || true
                python3 -m pip install --upgrade pip
                pip3 install -r .req_notorch.txt
              fi

              # 2) Build MIGraphX using benchmark-utils helper
              /migraphx/sh/build_migraphx.sh

              # 3) Export paths so python + libs can find MIGraphX
              export PYTHONPATH=/src/AMDMIGraphX/build/lib:${PYTHONPATH:-}
              export LD_LIBRARY_PATH=/src/AMDMIGraphX/build/lib:/opt/rocm/lib:${LD_LIBRARY_PATH:-}

              # 4) Ensure scripts are executable
              chmod +x tools/model_zoo/test_generator/test_models.sh || true
              chmod +x ./run-model-zoo-tests.js || true

              echo "Datasets at /datasets"
              node --version
              python3 --version

              # 5) Generate cases and run
              node ./run-model-zoo-tests.js --datasets /datasets --out tools/model_zoo/onnx_zoo
              mkdir -p "'"$ROOT"'/fp16" "'"$ROOT"'/fp32"
              bash tools/model_zoo/test_generator/test_models.sh tools/model_zoo/onnx_zoo 2>&1 | tee "'"$ROOT"'/raw.log"

              # 6) Copy per-model logs
              shopt -s nullglob
              for p in fp16 fp32; do
                src="tools/model_zoo/test_generator/logs/$p"
                [ -d "$src" ] && cp -f "$src"/*.log "'"$ROOT"'/$p/" || true
              done

              # 7) Summarize (write a tiny JS and run it)
              cat >/tmp/onnx_summarize.js << "JS"
              const fs=require("fs"),path=require("path");
              const runTs=process.env.RUN_TS;
              const rootDir=`${process.env.TEST_RESULTS_PATH}/onnx-${runTs}`;
              const precs=["fp32","fp16"];
              const summary={totals:{pass:0,fail:0},regressions:{}};
              const looksFailed=t=>/(Traceback \(most recent call last\)|\bERROR\b|AssertionError|Segmentation fault|^error:)/im.test(t);
              function failureMessage(t){
                const tb=[...t.matchAll(/Traceback \(most recent call last\):([\s\S]*?)(?:\n\s*\n|\Z)/g)];
                if(tb.length){
                  const block=tb[tb.length-1][1].trim().split(/\r?\n/).reverse();
                  for(const line of block) if(line.trim()) return line.trim();
                }
                const lines=t.split(/\r?\n/).map(l=>l.trim()).filter(Boolean).reverse();
                for(const l of lines) if (/(error|exception|failed|segmentation fault|assert)/i.test(l)) return l;
                return "failed (see log)";
              }
              for(const prec of precs){
                const d=path.join(rootDir,prec); const reg={passed:[],failed:[]};
                if(fs.existsSync(d)&&fs.statSync(d).isDirectory()){
                  const files=fs.readdirSync(d).filter(f=>f.endsWith(".log")).sort();
                  for(const fn of files){
                    const model=fn.slice(0,-4);
                    const txt=fs.readFileSync(path.join(d,fn),"utf8");
                    if(looksFailed(txt)){reg.failed.push({model,message:failureMessage(txt)});summary.totals.fail++;}
                    else {reg.passed.push(model);summary.totals.pass++;}
                  }
                }
                summary.regressions[prec]=reg;
              }
              fs.writeFileSync(path.join(rootDir,"summary.json"), JSON.stringify(summary,null,2));
              const lines=[];
              lines.push("## Totals",`- PASS: ${summary.totals.pass}`,`- FAIL: ${summary.totals.fail}`,"");
              for(const prec of precs){
                const reg=summary.regressions[prec];
                lines.push(`## ${prec.toUpperCase()}`);
                lines.push(`**Passed (${reg.passed.length})**`);
                if(reg.passed.length) reg.passed.forEach(m=>lines.push(`- ${m}`)); else lines.push("- none");
                lines.push("");
                lines.push(`**Failed (${reg.failed.length})**`);
                if(reg.failed.length) reg.failed.forEach(it=>lines.push(`- ${it.model}: \`${it.message}\``)); else lines.push("- none");
                lines.push("");
              }
              fs.writeFileSync(path.join(rootDir,"summary.md"), lines.join("\n"));
              JS
              node /tmp/onnx_summarize.js
            '

      - name: Upload logs and summary
        uses: actions/upload-artifact@v4
        with:
          name: model-zoo-logs-${{ steps.ts.outputs.stamp }}
          path: ${{ env.TEST_RESULTS_PATH }}/onnx-${{ steps.ts.outputs.stamp }}
          if-no-files-found: error
