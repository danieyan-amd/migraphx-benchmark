# File: migraphx-benchmark/.github/workflows/onnx-model.yaml
# Reusable workflow (called from AMDMIGraphX/.github/workflows/onnx.yaml)
# Mirrors the original Model Zoo Nightly behavior, but runs inside a thin Docker
# container and writes logs to the repository workspace (so artifacts can be uploaded).

name: ONNX Model Zoo (reusable)

on:
  workflow_call:
    inputs:
      rocm_release:
        description: ROCm Version (kept for interface parity)
        required: false
        type: string
        default: "6.4.2"
      onnx_reports_repo:
        description: Repo for optional results publishing (not required)
        required: false
        type: string
        default: ""
      benchmark_utils_repo:
        description: Unused here (kept for perf-parity)
        required: false
        type: string
        default: ""
      organization:
        description: Unused here (kept for perf-parity)
        required: false
        type: string
        default: "AMD"
      result_number:
        description: Unused here (kept for perf-parity)
        required: false
        type: string
        default: "10"
      model_timeout:
        description: Unused here (kept for interface parity)
        required: false
        type: string
        default: "30m"
      datasets_dir:
        description: Host path to datasets
        required: false
        type: string
        default: "/usr/share/migraphx/migraph_datasets"
      pt_tag:
        description: rocm/pytorch tag to use as runtime
        required: false
        type: string
        default: "latest"
      log_root:
        description: Logs output folder (in repo workspace)
        required: false
        type: string
        default: "logs"
    secrets:
      gh_token:
        required: true

permissions:
  contents: read

jobs:
  model-zoo-tests:
    runs-on: [self-hosted, Linux, X64]
    timeout-minutes: 120
    env:
      DATASETS_DIR: ${{ inputs.datasets_dir }}
      LOG_ROOT: ${{ inputs.log_root }}
      PT_TAG: ${{ inputs.pt_tag }}
      DOCKER_IMAGE: rocm/pytorch:${{ inputs.pt_tag }}
      # Expose MIGraphX libs from host
      PYTHONPATH: /opt/rocm/lib
      LD_LIBRARY_PATH: /opt/rocm/lib

    steps:
      - name: Checkout caller repo contents
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.gh_token }}

      - name: Init timestamp
        id: ts
        run: echo "stamp=$(date -u +%Y%m%dT%H%M%SZ)" >> "$GITHUB_OUTPUT"

      - name: Create host log dirs
        run: |
          set -e
          ROOT="${{ env.LOG_ROOT }}/${{ steps.ts.outputs.stamp }}"
          mkdir -p "${ROOT}/fp16" "${ROOT}/fp32"
          echo "${{ steps.ts.outputs.stamp }}" > "${ROOT}/run.timestamp"

      - name: Pull runtime image
        run: |
          set -euo pipefail
          echo "Pulling rocm/pytorch:${{ inputs.pt_tag }}"
          docker pull rocm/pytorch:${{ inputs.pt_tag }}

      - name: Run Model Zoo in Docker
        env:
          RUN_TS: ${{ steps.ts.outputs.stamp }}
        run: |
          set -euo pipefail

          if [ ! -d "${{ env.DATASETS_DIR }}" ]; then
            echo "Datasets dir not found on host: ${{ env.DATASETS_DIR }}"
            exit 1
          fi

          docker run --rm \
            --name model-zoo-${GITHUB_RUN_ID} \
            --device=/dev/kfd --device=/dev/dri --group-add=video \
            --network=host \
            -e TARGET=gpu \
            -e ATOL=0.001 -e RTOL=0.001 \
            -e RUN_TS="${RUN_TS}" \
            -e PYTHONPATH=${PYTHONPATH} \
            -e LD_LIBRARY_PATH=${LD_LIBRARY_PATH} \
            -v /opt/rocm:/opt/rocm:ro \
            -v "${{ env.DATASETS_DIR }}":/datasets:ro \
            -v "$GITHUB_WORKSPACE":/workspace \
            -w /workspace \
            "${{ env.DOCKER_IMAGE }}" \
            bash -lc '
              set -euo pipefail

              echo "Datasets at /datasets"
              # Install Node.js 20 for the JS generator
              if ! command -v node >/dev/null 2>&1; then
                apt-get update -y
                apt-get install -y --no-install-recommends ca-certificates curl gnupg
                install -d /etc/apt/keyrings
                curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg
                echo "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_20.x nodistro main" > /etc/apt/sources.list.d/nodesource.list
                apt-get update -y && apt-get install -y --no-install-recommends nodejs
              fi

              # Install test-only deps (skip torch/onnxruntime to not clash with base)
              if [ -f tools/model_zoo/test_generator/requirements.txt ]; then
                grep -viE "^(torch|torchvision|torchaudio|onnxruntime(-gpu)?)\\b" tools/model_zoo/test_generator/requirements.txt > .req_notorch.txt || true
                pip3 install -r .req_notorch.txt
              fi

              chmod +x tools/model_zoo/test_generator/test_models.sh || true
              chmod +x ./run-model-zoo-tests.js || true

              ROOT="${{ env.LOG_ROOT }}/${RUN_TS}"
              mkdir -p "${ROOT}/fp16" "${ROOT}/fp32"

              echo "Generating cases from datasets..."
              node ./run-model-zoo-tests.js \
                --datasets /datasets \
                --out tools/model_zoo/onnx_zoo

              echo "Running model-zoo tests..."
              bash tools/model_zoo/test_generator/test_models.sh tools/model_zoo/onnx_zoo 2>&1 | tee "${ROOT}/raw.log"

              shopt -s nullglob
              for p in fp16 fp32; do
                src="tools/model_zoo/test_generator/logs/$p"
                [ -d "$src" ] && cp -f "$src"/*.log "${ROOT}/$p/" || true
              done

              node - <<'"'"'JS'"'"'
              const fs = require('fs');
              const path = require('path');
              const runTs = process.env.RUN_TS;
              const root = path.join('logs', runTs);
              const precs = ['fp32','fp16'];
              const summary = { totals: { pass: 0, fail: 0 }, regressions: {} };
              function looksFailed(text){return /(Traceback \(most recent call last\)|\bERROR\b|AssertionError|Segmentation fault|^error:)/im.test(text);}
              function failureMessage(text){
                const tb=[...text.matchAll(/Traceback \(most recent call last\):([\s\S]*?)(?:\n\s*\n|\Z)/g)];
                if(tb.length){
                  const block=tb[tb.length-1][1].trim().split(/\r?\n/).reverse();
                  for(const line of block) if(line.trim()) return line.trim();
                }
                const lines=text.split(/\r?\n/).map(l=>l.trim()).filter(Boolean).reverse();
                for(const l of lines) if (/(error|exception|failed|segmentation fault|assert)/i.test(l)) return l;
                return 'failed (see log)';
              }
              for(const prec of precs){
                const d=path.join(root,prec); const reg={passed:[],failed:[]};
                if(fs.existsSync(d)&&fs.statSync(d).isDirectory()){
                  const files=fs.readdirSync(d).filter(f=>f.endsWith('.log')).sort();
                  for(const fn of files){
                    const model=fn.slice(0,-4);
                    const txt=fs.readFileSync(path.join(d,fn),'utf8');
                    if(looksFailed(txt)){reg.failed.push({model,message:failureMessage(txt)});summary.totals.fail++;}
                    else {reg.passed.push(model);summary.totals.pass++;}
                  }
                }
                summary.regressions[prec]=reg;
              }
              fs.writeFileSync(path.join(root,'summary.json'), JSON.stringify(summary,null,2));
              const lines=[];
              lines.push('## Totals');
              lines.push(`- PASS: ${summary.totals.pass}`);
              lines.push(`- FAIL: ${summary.totals.fail}`);
              lines.push('');
              for(const prec of precs){
                const reg=summary.regressions[prec];
                lines.push(`## ${prec.toUpperCase()}`);
                lines.push(`**Passed (${reg.passed.length})**`);
                if(reg.passed.length) reg.passed.forEach(m=>lines.push(`- ${m}`)); else lines.push('- none');
                lines.push('');
                lines.push(`**Failed (${reg.failed.length})**`);
                if(reg.failed.length) reg.failed.forEach(it=>lines.push(`- ${it.model}: \`${it.message}\``)); else lines.push('- none');
                lines.push('');
              }
              fs.writeFileSync(path.join(root,'summary.md'), lines.join('\n'));
              JS
            '

      - name: Upload logs and summary (timestamped)
        uses: actions/upload-artifact@v4
        with:
          name: model-zoo-logs-${{ steps.ts.outputs.stamp }}
          path: logs/${{ steps.ts.outputs.stamp }}
          if-no-files-found: error

---

# File: AMDMIGraphX/.github/workflows/onnx.yaml
# Caller workflow in AMDMIGraphX. It reads optional defaults (if you keep a config.md),
# then calls the reusable workflow above.

name: MIGraphX ONNX Model Tests

on:
  pull_request_target:
    branches: [develop]
    types: [opened, synchronize, closed]
  schedule:
    - cron: "0 7 * * 1-6"
  workflow_dispatch:
    inputs:
      rocm_release:
        description: ROCm Version
        required: true
        default: "6.4.2"
      onnx_reports_repo:
        description: Repository where ONNX reports are stored (optional)
        required: false
        default: ""
      benchmark_utils_repo:
        description: (unused) kept for parity
        required: false
        default: ""
      organization:
        description: (unused) kept for parity
        required: false
        default: "AMD"
      result_number:
        description: (unused) kept for parity
        required: false
        default: "10"
      model_timeout:
        description: (unused) kept for parity
        required: false
        default: "30m"
      datasets_dir:
        description: Host path to datasets
        required: false
        default: "/usr/share/migraphx/migraph_datasets"

concurrency:
  group: "onnx-${{ github.head_ref || github.base_ref || 'schedule' }}"
  cancel-in-progress: true

permissions:
  contents: read

jobs:
  call_reusable:
    runs-on: ubuntu-latest
    uses: danieyan-amd/migraphx-benchmark/.github/workflows/onnx-model.yaml@main
    with:
      rocm_release:        ${{ github.event.inputs.rocm_release      || '6.4.2' }}
      onnx_reports_repo:   ${{ github.event.inputs.onnx_reports_repo || '' }}
      benchmark_utils_repo:${{ github.event.inputs.benchmark_utils_repo || '' }}
      organization:        ${{ github.event.inputs.organization      || 'AMD' }}
      result_number:       ${{ github.event.inputs.result_number     || '10' }}
      model_timeout:       ${{ github.event.inputs.model_timeout     || '30m' }}
      datasets_dir:        ${{ github.event.inputs.datasets_dir      || '/usr/share/migraphx/migraph_datasets' }}
    secrets:
      gh_token: ${{ secrets.MIGRAPHX_BOT_TOKEN }}
