name: MIGraphX ONNX Model Tests (reusable, thin PyTorch image)

on:
  workflow_call:
    inputs:
      rocm_release:
        description: ROCm Version (for parity with caller; not used by image)
        type: string
        required: true
      onnx_reports_repo:
        description: Repository where ONNX reports are stored (optional use downstream)
        type: string
        required: true
      benchmark_utils_repo:
        description: Repo for utils (accepted for parity; not used here)
        type: string
        required: true
      organization:
        description: Organization (used for paths if needed)
        type: string
        required: true
      result_number:
        description: Last N results (accepted for parity)
        type: string
        required: true
      model_timeout:
        description: Per-model timeout (accepted for parity)
        type: string
        required: true
    secrets:
      gh_token:
        description: GitHub token (optional; used when checking out reports repo)
        required: false

permissions:
  contents: read

env:
  DATASETS_DIR: /usr/share/migraphx/migraph_datasets
  LOG_ROOT: logs

  # Image knob â€” we stick to a thin public ROCm PyTorch image
  PT_TAG: latest
  DOCKER_IMAGE: "rocm/pytorch:latest"

  # Keep these in env so container inherits
  PYTHONPATH: /opt/rocm/lib
  LD_LIBRARY_PATH: /opt/rocm/lib

jobs:
  model-zoo-tests:
    runs-on: [self-hosted, Linux, X64]
    timeout-minutes: 90

    steps:
      - name: Checkout sources
        uses: actions/checkout@v4

      - name: Init timestamp
        id: ts
        run: echo "stamp=$(date -u +%Y%m%dT%H%M%SZ)" >> "$GITHUB_OUTPUT"

      - name: Choose results directory
        id: results
        shell: bash
        run: |
          set -euo pipefail
          RUN_TS="${{ steps.ts.outputs.stamp }}"
          # Store results OUTSIDE the repo to avoid permission/cleanup issues
          RES_DIR="${RUNNER_TEMP}/onnx-${RUN_TS}"
          mkdir -p "$RES_DIR/fp16" "$RES_DIR/fp32"
          echo "run_ts=$RUN_TS" >> "$GITHUB_OUTPUT"
          echo "host_results=$RES_DIR" >> "$GITHUB_OUTPUT"
          echo "Using $RES_DIR for artifacts"

      - name: List datasets on host (debug)
        shell: bash
        run: |
          set -e
          for d in \
            /usr/share/migraphx/migraph_datasets \
            /usr/share/migraphx/saved-models \
            /usr/share/migraphx/new-saved-models
          do
            echo "==> Host path: $d"
            if [ -d "$d" ]; then
              if find "$d" -type f \( -name '*.onnx' -o -name '*.pb' \) -print -quit | grep -q . ; then
                echo "    contains model files"
              else
                echo "    (no .onnx/.pb files found)"
              fi
            else
              echo "    (missing)"
            fi
          done

      - name: Run Model Zoo tests in container
        env:
          RUN_TS: ${{ steps.results.outputs.run_ts }}
        shell: bash
        run: |
          set -euo pipefail

          if [ ! -d "${DATASETS_DIR}" ]; then
            echo "Datasets dir not found on host: ${DATASETS_DIR}"
            exit 1
          fi

          # Guard: bail out fast if no models are present on host
          if ! find "${DATASETS_DIR}" -type f \( -name '*.onnx' -o -name '*.pb' \) -print -quit | grep -q . ; then
            echo "No .onnx/.pb files under ${DATASETS_DIR}. Skipping container run."
            mkdir -p "${{ steps.results.outputs.host_results }}"
            printf '## Totals\n- PASS: 0\n- FAIL: 0\n' > "${{ steps.results.outputs.host_results }}/summary.md"
            printf '{"totals":{"pass":0,"fail":0},"regressions":{"fp32":{"passed":[],"failed":[]},"fp16":{"passed":[],"failed":[]}}}\n' > "${{ steps.results.outputs.host_results }}/summary.json"
            exit 0
          fi

          # Write the in-container script to a temp file on the host and mount it
          cat <<'INCON' > /tmp/in_container.sh
          set -euo pipefail

          echo "Datasets at /datasets"

          # ---- Node.js for the JS generator ----
          if ! command -v node >/dev/null 2>&1; then
            apt-get update -y
            apt-get install -y --no-install-recommends ca-certificates curl gnupg
            install -d /etc/apt/keyrings
            curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg
            echo "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_20.x nodistro main" > /etc/apt/sources.list.d/nodesource.list
            apt-get update -y && apt-get install -y --no-install-recommends nodejs
          fi

          # ---- Install test-only deps (avoid torch/ort conflicts) ----
          if [ -f tools/model_zoo/test_generator/requirements.txt ]; then
            grep -viE "^(torch|torchvision|torchaudio|onnxruntime(-gpu)?)\\b" tools/model_zoo/test_generator/requirements.txt > .req_notorch.txt || true
            pip3 install -r .req_notorch.txt
          fi

          chmod +x tools/model_zoo/test_generator/test_models.sh || true
          chmod +x ./run-model-zoo-tests.js || true

          echo "Checking dataset candidates inside container..."
          for d in /usr/share/migraphx/migraph_datasets /datasets /saved-models /new-saved-models; do
            if [ -d "$d" ]; then
              if find "$d" -type f \( -name "*.onnx" -o -name "*.pb" \) -print -quit | grep -q . ; then
                printf "  %s: present (has model files)\n" "$d"
              else
                printf "  %s: present (no model files)\n" "$d"
              fi
            else
              printf "  %s: missing\n" "$d"
            fi
          done

          # Prefer the canonical MIGraphX datasets path if present, else fall back to /datasets
          if [ -d /usr/share/migraphx/migraph_datasets ]; then
            DATASET_ROOT=/usr/share/migraphx/migraph_datasets
          else
            DATASET_ROOT=/datasets
          fi

          echo "Running model-zoo tests from ${DATASET_ROOT}..."
          bash tools/model_zoo/test_generator/test_models.sh "${DATASET_ROOT}" 2>&1 | tee "/results/raw.log"

          shopt -s nullglob
          mkdir -p /results/fp16 /results/fp32
          for p in fp16 fp32; do
            src="tools/model_zoo/test_generator/logs/$p"
            if [ -d "$src" ]; then
              files=( "$src"/*.log )
              if [ -e "${files[0]}" ]; then
                cp -f "${files[@]}" "/results/$p/"
              fi
            fi
          done

          # ---- Summarize with Node (literal heredoc) ----
          cat <<'EOF' >/tmp/onnx_summarize.js
          const fs = require('fs');
          const path = require('path');
          const root = '/results';
          const precs = ['fp32','fp16'];
          const summary = { totals: { pass: 0, fail: 0 }, regressions: {} };
          function looksFailed(text){return /(Traceback \(most recent call last\)|\bERROR\b|AssertionError|Segmentation fault|^error:)/im.test(text);}
          function failureMessage(text){
            const tb=[...text.matchAll(/Traceback \(most recent call last\):([\s\S]*?)(?:\n\s*\n|\Z)/g)];
            if(tb.length){
              const block=tb[tb.length-1][1].trim().split(/\r?\n/).reverse();
              for(const line of block) if(line.trim()) return line.trim();
            }
            const lines=text.split(/\r?\n/).map(l=>l.trim()).filter(Boolean).reverse();
            for(const l of lines) if (/(error|exception|failed|segmentation fault|assert)/i.test(l)) return l;
            return 'failed (see log)';
          }
          for(const prec of precs){
            const d=path.join(root,prec); const reg={passed:[],failed:[]};
            if(fs.existsSync(d)&&fs.statSync(d).isDirectory()){
              const files=fs.readdirSync(d).filter(f=>f.endsWith('.log')).sort();
              for(const fn of files){
                const model=fn.slice(0,-4);
                const txt=fs.readFileSync(path.join(d,fn),'utf8');
                if(looksFailed(txt)){reg.failed.push({model,message:failureMessage(txt)});summary.totals.fail++;}
                else {reg.passed.push(model);summary.totals.pass++;}
              }
            }
            summary.regressions[prec]=reg;
          }
          fs.writeFileSync(path.join(root,'summary.json'), JSON.stringify(summary,null,2));
          const lines=[];
          lines.push('## Totals');
          lines.push(`- PASS: ${summary.totals.pass}`);
          lines.push(`- FAIL: ${summary.totals.fail}`);
          lines.push('');
          for(const prec of precs){
            const reg=summary.regressions[prec];
            lines.push(`## ${prec.toUpperCase()}`);
            lines.push(`**Passed (${reg.passed.length})**`);
            if(reg.passed.length) reg.passed.forEach(m=>lines.push(`- ${m}`)); else lines.push('- none');
            lines.push('');
            lines.push(`**Failed (${reg.failed.length})**`);
            if(reg.failed.length) reg.failed.forEach(it=>lines.push(`- ${it.model}: \`${it.message}\``)); else lines.push('- none');
            lines.push('');
          }
          fs.writeFileSync(path.join(root,'summary.md'), lines.join('\n'));
          EOF

          node /tmp/onnx_summarize.js
          INCON

          chmod +x /tmp/in_container.sh

          docker run --rm \
            --name model-zoo-${GITHUB_RUN_ID} \
            --device=/dev/kfd --device=/dev/dri --group-add=video \
            --network=host \
            -e TARGET=gpu \
            -e ATOL=0.001 -e RTOL=0.001 \
            -e RUN_TS="${RUN_TS}" \
            -e PYTHONPATH=${PYTHONPATH} \
            -e LD_LIBRARY_PATH=${LD_LIBRARY_PATH} \
            -v /opt/rocm:/opt/rocm:ro \
            -v "${DATASETS_DIR}":/datasets:ro \
            -v "${DATASETS_DIR}":/usr/share/migraphx/migraph_datasets:ro \
            
            -v "${{ steps.results.outputs.host_results }}":/results \
            -v "$GITHUB_WORKSPACE":/workspace \
            -v /tmp/in_container.sh:/in_container.sh:ro \
            -w /workspace \
            "${DOCKER_IMAGE}" \
            bash /in_container.sh

      - name: Upload logs and summary (timestamped)
        uses: actions/upload-artifact@v4
        with:
          name: model-zoo-logs-${{ steps.results.outputs.run_ts }}
          path: ${{ steps.results.outputs.host_results }}
          if-no-files-found: error
