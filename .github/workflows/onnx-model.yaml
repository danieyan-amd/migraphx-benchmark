name: ONNX Model Zoo Tests (PyTorch + MIGraphX)

on:
  workflow_call:
    inputs:
      rocm_release:
        required: true
        type: string            # e.g. "6.4.2"
      pytorch_base_tag:
        required: false
        type: string            # overrideable if you need a specific rocm/pytorch tag
        default: "rocm6.4.2_ubuntu22.04_py3.10_pytorch_2.4.0"
      benchmark_utils_repo:
        required: true
        type: string            # e.g. "ROCm/migraphx-benchmark-utils" or your fork
      onnx_reports_repo:
        required: true
        type: string
      organization:
        required: true
        type: string            # "AMD" or "HTEC"
      result_number:
        required: true
        type: string
      model_timeout:
        required: true
        type: string            # "30m"
    secrets:
      gh_token:
        required: true
      mail_user:
        required: false
      mail_pass:
        required: false

permissions:
  contents: read

env:
  UTILS_DIR: benchmark-utils
  REPORTS_DIR: migraphx-reports
  DATASETS_DIR: /usr/share/migraphx/saved-models
  TEST_RESULTS_PATH: /usr/share/migraphx/${{ inputs.organization }}/test-results
  REPORTS_PATH: /usr/share/migraphx/${{ inputs.organization }}/reports
  MAIL_TO: ""
  MAIL_CC: ""
  MAIL_FROM: GH Actions
  MAIL_SUBJECT: Nightly ONNX run
  MAIL_BODY: Scheduled ONNX run on develop branch

jobs:
  onnx_zoo:
    runs-on: [self-hosted, Linux, X64]   # your gpu942f runner is fine
    timeout-minutes: 120

    steps:
      - name: Map runner to GPU/CPU
        shell: bash
        run: |
          case "${RUNNER_NAME:-unknown}" in
            GPU942F_1|gpu942f) GPU=0; CPU="0-63" ;;
            GPU942F_2)         GPU=4; CPU="64-127" ;;
            *) echo "Unknown runner ${RUNNER_NAME:-?}, defaulting GPU=0, all CPUs"; GPU=0; CPU="0-$(( $(nproc)-1 ))" ;;
          esac
          echo "GPU=$GPU" >> $GITHUB_ENV
          echo "CPU=$CPU" >> $GITHUB_ENV

      - name: Checkout AMDMIGraphX code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.gh_token }}

      - name: Checkout benchmark utils
        uses: actions/checkout@v4
        with:
          repository: ${{ inputs.benchmark_utils_repo }}
          path: ${{ env.UTILS_DIR }}
          token: ${{ secrets.gh_token }}

      - name: Write lightweight Dockerfile (PyTorch+ROCm base)
        run: |
          cat > Dockerfile.onnx << 'DOCKER'
          ARG PT_TAG
          FROM rocm/pytorch:${PT_TAG}

          # Basics + Node.js (JS generator), jq, bash tools
          RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
               ca-certificates curl gnupg build-essential git cmake pkg-config python3-pip python3-dev \
               jq bash coreutils findutils sed gawk \
            && install -d /etc/apt/keyrings \
            && curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg \
            && echo 'deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_20.x nodistro main' > /etc/apt/sources.list.d/nodesource.list \
            && apt-get update && apt-get install -y --no-install-recommends nodejs \
            && rm -rf /var/lib/apt/lists/*

          # Create workdir
          WORKDIR /workspace
          DOCKER

      - name: Build ONNX runner image
        run: |
          docker build \
            --build-arg PT_TAG=${{ inputs.pytorch_base_tag }} \
            -t onnx-pytorch-migraphx:${{ github.sha }} \
            -f Dockerfile.onnx .

      - name: Init timestamp
        id: ts
        run: echo "stamp=$(date -u +%Y%m%dT%H%M%SZ)" >> "$GITHUB_OUTPUT"

      - name: Run ONNX tests in container
        env:
          RUN_TS: ${{ steps.ts.outputs.stamp }}
        shell: bash
        run: |
          set -euo pipefail
          ROOT="${{ env.TEST_RESULTS_PATH }}/onnx-${RUN_TS}"
          mkdir -p "$ROOT/fp16" "$ROOT/fp32"

          docker run --rm \
            --name onnx-model-${GITHUB_RUN_ID} \
            --cpuset-cpus=${{ env.CPU }} \
            -e ROCR_VISIBLE_DEVICES=${{ env.GPU }} \
            --device=/dev/dri --device=/dev/kfd \
            --group-add=video --network=host \
            -e RUN_TS=${RUN_TS} \
            -v "$PWD:/workspace:rw" \
            -v "${{ env.DATASETS_DIR }}:/datasets:ro" \
            -v "${{ env.TEST_RESULTS_PATH }}:${{ env.TEST_RESULTS_PATH }}" \
            -v "$PWD/${{ env.UTILS_DIR }}/scripts:/migraphx/sh:ro" \
            onnx-pytorch-migraphx:${{ github.sha }} \
            bash -lc '
              set -euo pipefail
              cd /workspace

              # --- Install Python deps (filter out torch/onnxruntime-gpu) ---
              if [ -f tools/model_zoo/test_generator/requirements.txt ]; then
                grep -viE "^(torch|torchvision|torchaudio|onnxruntime-gpu)\b" tools/model_zoo/test_generator/requirements.txt > .req_notorch.txt || true
                python3 -m pip install --upgrade pip
                pip3 install -r .req_notorch.txt
              fi

              # --- Build MIGraphX (uses benchmark-utils script) ---
              /migraphx/sh/build_migraphx.sh

              # --- Export so Python can import + find libs ---
              export PYTHONPATH=/src/AMDMIGraphX/build/lib:${PYTHONPATH:-}
              export LD_LIBRARY_PATH=/src/AMDMIGraphX/build/lib:/opt/rocm/lib:${LD_LIBRARY_PATH:-}

              # --- Make scripts runnable ---
              chmod +x tools/model_zoo/test_generator/test_models.sh || true
              chmod +x ./run-model-zoo-tests.js || true

              echo "Datasets at /datasets"
              node --version
              python3 --version

              # --- Generate cases + run tests ---
              node ./run-model-zoo-tests.js --datasets /datasets --out tools/model_zoo/onnx_zoo

              mkdir -p "'"$ROOT"'/fp16" "'"$ROOT"'/fp32"
              bash tools/model_zoo/test_generator/test_models.sh tools/model_zoo/onnx_zoo 2>&1 | tee "'"$ROOT"'/raw.log"

              # Copy per-model logs back into the timestamped folder
              shopt -s nullglob
              for p in fp16 fp32; do
                src="tools/model_zoo/test_generator/logs/$p"
                [ -d "$src" ] && cp -f "$src"/*.log "'"$ROOT"'/$p/" || true
              done

              # Summarize (write a tiny JS file to avoid heredoc quoting issues)
              cat >/tmp/onnx_summarize.js << "JS"
              const fs=require("fs"),path=require("path");
              const runTs=process.env.RUN_TS;
              const rootDir=`${process.env.TEST_RESULTS_PATH}/onnx-${runTs}`;
              const precs=["fp32","fp16"];
              const summary={totals:{pass:0,fail:0},regressions:{}};
              const looksFailed=t=>/(Traceback \(most recent call last\)|\bERROR\b|AssertionError|Segmentation fault|^error:)/im.test(t);
              function failureMessage(t){
                const tb=[...t.matchAll(/Traceback \(most recent call last\):([\s\S]*?)(?:\n\s*\n|\Z)/g)];
                if(tb.length){
                  const block=tb[tb.length-1][1].trim().split(/\r?\n/).reverse();
                  for(const line of block) if(line.trim()) return line.trim();
                }
                const lines=t.split(/\r?\n/).map(l=>l.trim()).filter(Boolean).reverse();
                for(const l of lines) if (/(error|exception|failed|segmentation fault|assert)/i.test(l)) return l;
                return "failed (see log)";
              }
              for(const prec of precs){
                const d=path.join(rootDir,prec); const reg={passed:[],failed:[]};
                if(fs.existsSync(d)&&fs.statSync(d).isDirectory()){
                  const files=fs.readdirSync(d).filter(f=>f.endsWith(".log")).sort();
                  for(const fn of files){
                    const model=fn.slice(0,-4);
                    const txt=fs.readFileSync(path.join(d,fn),"utf8");
                    if(looksFailed(txt)){reg.failed.push({model,message:failureMessage(txt)});summary.totals.fail++;}
                    else {reg.passed.push(model);summary.totals.pass++;}
                  }
                }
                summary.regressions[prec]=reg;
              }
              fs.writeFileSync(path.join(rootDir,"summary.json"), JSON.stringify(summary,null,2));
              const lines=[];
              lines.push("## Totals",`- PASS: ${summary.totals.pass}`,`- FAIL: ${summary.totals.fail}`,"");
              for(const prec of precs){
                const reg=summary.regressions[prec];
                lines.push(`## ${prec.toUpperCase()}`);
                lines.push(`**Passed (${reg.passed.length})**`);
                if(reg.passed.length) reg.passed.forEach(m=>lines.push(`- ${m}`)); else lines.push("- none");
                lines.push("");
                lines.push(`**Failed (${reg.failed.length})**`);
                if(reg.failed.length) reg.failed.forEach(it=>lines.push(`- ${it.model}: \`${it.message}\``)); else lines.push("- none");
                lines.push("");
              }
              fs.writeFileSync(path.join(rootDir,"summary.md"), lines.join("\n"));
              JS
              node /tmp/onnx_summarize.js
            '

      - name: Upload logs and summary (timestamped)
        uses: actions/upload-artifact@v4
        with:
          name: model-zoo-logs-${{ steps.ts.outputs.stamp }}
          path: ${{ env.TEST_RESULTS_PATH }}/onnx-${{ steps.ts.outputs.stamp }}
          if-no-files-found: error
